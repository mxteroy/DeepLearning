{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to vectorize the labels: you can cast the label list as an integer tensor or you can use one-hot encoding\n",
    "# one-hot encoding is a popular format for categorical data, also called categorical encoding\n",
    "\n",
    "def to_one_hot (labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1\n",
    "    return results\n",
    "\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#larger dimensional layers are required to classify the inputs into one of 46 categories without loss of data in the layers\n",
    "# a 16-dimensional layer might be too limited to learn 46 different classes: such small layers might act as information bottlenecks\n",
    "#so I'm going to use a 64-dimensional layer\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))#46 dimensional-layer output a 46-dimensional vector for the different output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 252us/step - loss: 2.4975 - accuracy: 0.5371 - val_loss: 1.6827 - val_accuracy: 0.6610\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 105us/step - loss: 1.3858 - accuracy: 0.7124 - val_loss: 1.3086 - val_accuracy: 0.7230\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 105us/step - loss: 1.0520 - accuracy: 0.7784 - val_loss: 1.1660 - val_accuracy: 0.7510\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 104us/step - loss: 0.8360 - accuracy: 0.8231 - val_loss: 1.0530 - val_accuracy: 0.7810\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 105us/step - loss: 0.6696 - accuracy: 0.8583 - val_loss: 0.9795 - val_accuracy: 0.8070\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 105us/step - loss: 0.5341 - accuracy: 0.8913 - val_loss: 0.9693 - val_accuracy: 0.8000\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 112us/step - loss: 0.4321 - accuracy: 0.9089 - val_loss: 0.9261 - val_accuracy: 0.8180\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 105us/step - loss: 0.3518 - accuracy: 0.9257 - val_loss: 0.9113 - val_accuracy: 0.8240\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 104us/step - loss: 0.2906 - accuracy: 0.9386 - val_loss: 0.9320 - val_accuracy: 0.8130\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 105us/step - loss: 0.2476 - accuracy: 0.9426 - val_loss: 0.9288 - val_accuracy: 0.8110\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 105us/step - loss: 0.2119 - accuracy: 0.9480 - val_loss: 0.9294 - val_accuracy: 0.8150\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 107us/step - loss: 0.1869 - accuracy: 0.9498 - val_loss: 0.9553 - val_accuracy: 0.8170\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 107us/step - loss: 0.1655 - accuracy: 0.9530 - val_loss: 0.9439 - val_accuracy: 0.8160\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 107us/step - loss: 0.1539 - accuracy: 0.9558 - val_loss: 0.9728 - val_accuracy: 0.8130\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 106us/step - loss: 0.1423 - accuracy: 0.9541 - val_loss: 0.9914 - val_accuracy: 0.8080\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 109us/step - loss: 0.1300 - accuracy: 0.9557 - val_loss: 1.0102 - val_accuracy: 0.8120\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 113us/step - loss: 0.1286 - accuracy: 0.9562 - val_loss: 1.0365 - val_accuracy: 0.8070\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 109us/step - loss: 0.1195 - accuracy: 0.9578 - val_loss: 1.0961 - val_accuracy: 0.8000\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 106us/step - loss: 0.1198 - accuracy: 0.9560 - val_loss: 1.0903 - val_accuracy: 0.8070\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 105us/step - loss: 0.1121 - accuracy: 0.9587 - val_loss: 1.1009 - val_accuracy: 0.8050\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train, partial_y_train, validation_data=(x_val, y_val), epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
